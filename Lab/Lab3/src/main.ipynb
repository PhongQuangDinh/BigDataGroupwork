{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LAB3: SPARK STREAMING</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inormation team:</h2>\n",
    " 21127456 - Vo Cao Tri <br>\n",
    " 21127608 - Tran Trung Hieu <br>\n",
    " 21127668 - Dinh Quang Phong <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>IMPORT LIBRARY</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "# Creating a SparkSession in Python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\")\\\n",
    "          .appName(\"Spark Streaming Demonstration\")\\\n",
    "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "          .getOrCreate()\n",
    "# keep the size of shuffles small\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 1: Discover a method to simulate a stream by utilizing data sourced from files</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define input data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input path in local filesystem\n",
    "inputPath = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get schema from file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticInputDF = (\n",
    "  spark\n",
    "    .read    \n",
    "    .csv(inputPath)\n",
    ")\n",
    "schema = staticInputDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('_c0', StringType(), True), StructField('_c1', StringType(), True), StructField('_c2', StringType(), True), StructField('_c3', StringType(), True), StructField('_c4', StringType(), True), StructField('_c5', StringType(), True), StructField('_c6', StringType(), True), StructField('_c7', StringType(), True), StructField('_c8', StringType(), True), StructField('_c9', StringType(), True), StructField('_c10', StringType(), True), StructField('_c11', StringType(), True), StructField('_c12', StringType(), True), StructField('_c13', StringType(), True), StructField('_c14', StringType(), True), StructField('_c15', StringType(), True), StructField('_c16', StringType(), True), StructField('_c17', StringType(), True), StructField('_c18', StringType(), True), StructField('_c19', StringType(), True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n"
     ]
    }
   ],
   "source": [
    "entries = os.listdir(\"../data\")\n",
    "    \n",
    "    # Filter out directories, leaving only files\n",
    "files = [entry for entry in entries if os.path.isfile(os.path.join(\"../data\", entry))]\n",
    "\n",
    "number_file = len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create spark streaming and check the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .schema(schema)                # Set the schema of the csv data\n",
    "    .option(\"maxFilesPerTrigger\", number_file)  # Treat a sequence of files as a stream by picking one file at a time\n",
    "    .csv(inputPath)\n",
    ")\n",
    "# cast the Timestamp type since it is not automatically parsed\n",
    "streamingInputDF = streamingInputDF.select(f.col('_c0').alias('Type'), f.col('_c3').alias('Time'))\n",
    "# Same query as staticInputDF\n",
    "streamingCountsDF = (                 \n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      window(streamingInputDF.Time, \"1 hour\"))\n",
    "    .count()\n",
    ")\n",
    "# Is this DF actually a streaming DF?\n",
    "print('Using Spark streaming:', streamingCountsDF.isStreaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 2: Query that aggregates the number of trips by drop-off datetime for each hour. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query stores the aggregation results in memory then visualize it\n",
    "strQuery = (\n",
    "  streamingCountsDF\n",
    "    .writeStream\n",
    "    .format(\"memory\")      \n",
    "    .queryName(\"counts\")   \n",
    "    .outputMode(\"complete\") \n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "strQuery.awaitTermination(60)\n",
    "\n",
    "strQuery.stop()\n",
    "\n",
    "result = spark.sql('select * from counts order by window')\n",
    "\n",
    "result.show(result.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create folders and save the result of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countIncrement = 360000\n",
    "count = countIncrement\n",
    "\n",
    "for row in result.collect():\n",
    "    path = f'./output-{count}'\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    with open(f\"{path}/output-{count}.txt\", \"w\") as file:\n",
    "        file.write(str(row['count']))\n",
    "        \n",
    "    count += countIncrement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Task 3: Create a query that counts the number of taxi trips each hour that drop off at either the Goldman Sachs headquarters or the Citigroup headquarters.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the map location of Goldman Sachs and Citygroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldmanCondition = (\n",
    "    (col(\"dropoff_longitude\") >= -74.0144185) & (col(\"dropoff_longitude\") <= -74.013777) &\n",
    "    (col(\"dropoff_latitude\") >= 40.7138745) & (col(\"dropoff_latitude\") <= 40.7152275)\n",
    ")\n",
    "\n",
    "citigroupCondition = (\n",
    "    (col(\"dropoff_longitude\") >= -74.012083) & (col(\"dropoff_longitude\") <= -74.009867) &\n",
    "    (col(\"dropoff_latitude\") >= 40.720053) & (col(\"dropoff_latitude\") <= 40.7217236)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create spark streaming with schema of input data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .schema(schema)          \n",
    "    .csv(inputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Get the column that need to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellowRecordsDF = streamingInputDF.filter(f.col('_c0') == 'yellow')\n",
    "yellowRecordsDF = yellowRecordsDF.select(f.col('_c0').alias('Type'),f.col('_c10').alias('dropoff_longitude'), f.col('_c11').alias('dropoff_latitude'), f.col('_c3').alias('dropoff_datetime'))\n",
    "\n",
    "greenRecordsDF = streamingInputDF.filter(f.col('_c0') == 'green')\n",
    "greenRecordsDF = greenRecordsDF.select(f.col('_c0').alias('Type'),f.col('_c8').alias('dropoff_longitude'), f.col('_c9').alias('dropoff_latitude'), f.col('_c3').alias('dropoff_datetime'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Filter the taxi trips that drop-off either Goldman Sachs headquarters or the Citigroup headquarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yellowRecordsDF.union(greenRecordsDF)\n",
    "\n",
    "goldmanDF = df.filter(goldmanCondition).withColumn(\"headquarters\", lit(\"goldman\"))\n",
    "citigroupDF = df.filter(citigroupCondition).withColumn(\"headquarters\", lit(\"citigroup\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDF = goldmanDF.union(citigroupDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Count the number of trips using spark streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingCount = (                 \n",
    "    filteredDF\n",
    "    .groupBy( \n",
    "      filteredDF.Type,\n",
    "      window(filteredDF.dropoff_datetime, \"1 hour\"), filteredDF.headquarters)\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# This query stores the aggregation results in memory then visualize it\n",
    "query = (\n",
    "  streamingCount\n",
    "    .writeStream\n",
    "    .format(\"memory\")         # console or memory(= store in-memory table) \n",
    "    .queryName(\"counts\")      # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")   \n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "query.awaitTermination(60)\n",
    "\n",
    "query.stop()\n",
    "\n",
    "result = spark.sql('select * from counts order by window')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------+-----+\n",
      "|Action|              window|headquarters|count|\n",
      "+------+--------------------+------------+-----+\n",
      "|yellow|{2015-12-01 00:00...|   citigroup|    6|\n",
      "|yellow|{2015-12-01 01:00...|   citigroup|    2|\n",
      "|yellow|{2015-12-01 02:00...|   citigroup|    2|\n",
      "|yellow|{2015-12-01 03:00...|   citigroup|    2|\n",
      "|yellow|{2015-12-01 04:00...|   citigroup|    1|\n",
      "|yellow|{2015-12-01 05:00...|   citigroup|   11|\n",
      "|yellow|{2015-12-01 05:00...|     goldman|    8|\n",
      "|yellow|{2015-12-01 06:00...|     goldman|   28|\n",
      "|yellow|{2015-12-01 06:00...|   citigroup|   70|\n",
      "|yellow|{2015-12-01 07:00...|   citigroup|   93|\n",
      "|yellow|{2015-12-01 07:00...|     goldman|   44|\n",
      "| green|{2015-12-01 07:00...|   citigroup|    2|\n",
      "|yellow|{2015-12-01 08:00...|   citigroup|   75|\n",
      "| green|{2015-12-01 08:00...|   citigroup|    1|\n",
      "|yellow|{2015-12-01 08:00...|     goldman|   59|\n",
      "| green|{2015-12-01 09:00...|     goldman|    2|\n",
      "|yellow|{2015-12-01 09:00...|   citigroup|   75|\n",
      "|yellow|{2015-12-01 09:00...|     goldman|   70|\n",
      "|yellow|{2015-12-01 10:00...|   citigroup|   32|\n",
      "| green|{2015-12-01 10:00...|   citigroup|    1|\n",
      "+------+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------+------------+-----+\n",
      "|Action|window                                    |headquarters|count|\n",
      "+------+------------------------------------------+------------+-----+\n",
      "|yellow|{2015-12-01 05:00:00, 2015-12-01 06:00:00}|goldman     |8    |\n",
      "|yellow|{2015-12-01 06:00:00, 2015-12-01 07:00:00}|goldman     |28   |\n",
      "|yellow|{2015-12-01 07:00:00, 2015-12-01 08:00:00}|goldman     |44   |\n",
      "|yellow|{2015-12-01 08:00:00, 2015-12-01 09:00:00}|goldman     |59   |\n",
      "|green |{2015-12-01 09:00:00, 2015-12-01 10:00:00}|goldman     |2    |\n",
      "|yellow|{2015-12-01 09:00:00, 2015-12-01 10:00:00}|goldman     |70   |\n",
      "|yellow|{2015-12-01 10:00:00, 2015-12-01 11:00:00}|goldman     |56   |\n",
      "|green |{2015-12-01 10:00:00, 2015-12-01 11:00:00}|goldman     |2    |\n",
      "|green |{2015-12-01 11:00:00, 2015-12-01 12:00:00}|goldman     |2    |\n",
      "|yellow|{2015-12-01 11:00:00, 2015-12-01 12:00:00}|goldman     |32   |\n",
      "|yellow|{2015-12-01 12:00:00, 2015-12-01 13:00:00}|goldman     |26   |\n",
      "|yellow|{2015-12-01 13:00:00, 2015-12-01 14:00:00}|goldman     |19   |\n",
      "|yellow|{2015-12-01 14:00:00, 2015-12-01 15:00:00}|goldman     |31   |\n",
      "|yellow|{2015-12-01 15:00:00, 2015-12-01 16:00:00}|goldman     |22   |\n",
      "|green |{2015-12-01 16:00:00, 2015-12-01 17:00:00}|goldman     |1    |\n",
      "|yellow|{2015-12-01 16:00:00, 2015-12-01 17:00:00}|goldman     |11   |\n",
      "|yellow|{2015-12-01 17:00:00, 2015-12-01 18:00:00}|goldman     |3    |\n",
      "|green |{2015-12-01 18:00:00, 2015-12-01 19:00:00}|goldman     |1    |\n",
      "|yellow|{2015-12-01 18:00:00, 2015-12-01 19:00:00}|goldman     |4    |\n",
      "|yellow|{2015-12-01 19:00:00, 2015-12-01 20:00:00}|goldman     |4    |\n",
      "+------+------------------------------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goldman_res = result.filter(f.col('headquarters') == 'goldman')\n",
    "goldman_res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------+------------+-----+\n",
      "|Action|window                                    |headquarters|count|\n",
      "+------+------------------------------------------+------------+-----+\n",
      "|yellow|{2015-12-01 00:00:00, 2015-12-01 01:00:00}|citigroup   |6    |\n",
      "|yellow|{2015-12-01 01:00:00, 2015-12-01 02:00:00}|citigroup   |2    |\n",
      "|yellow|{2015-12-01 02:00:00, 2015-12-01 03:00:00}|citigroup   |2    |\n",
      "|yellow|{2015-12-01 03:00:00, 2015-12-01 04:00:00}|citigroup   |2    |\n",
      "|yellow|{2015-12-01 04:00:00, 2015-12-01 05:00:00}|citigroup   |1    |\n",
      "|yellow|{2015-12-01 05:00:00, 2015-12-01 06:00:00}|citigroup   |11   |\n",
      "|yellow|{2015-12-01 06:00:00, 2015-12-01 07:00:00}|citigroup   |70   |\n",
      "|yellow|{2015-12-01 07:00:00, 2015-12-01 08:00:00}|citigroup   |93   |\n",
      "|green |{2015-12-01 07:00:00, 2015-12-01 08:00:00}|citigroup   |2    |\n",
      "|yellow|{2015-12-01 08:00:00, 2015-12-01 09:00:00}|citigroup   |75   |\n",
      "|green |{2015-12-01 08:00:00, 2015-12-01 09:00:00}|citigroup   |1    |\n",
      "|yellow|{2015-12-01 09:00:00, 2015-12-01 10:00:00}|citigroup   |75   |\n",
      "|green |{2015-12-01 10:00:00, 2015-12-01 11:00:00}|citigroup   |1    |\n",
      "|yellow|{2015-12-01 10:00:00, 2015-12-01 11:00:00}|citigroup   |32   |\n",
      "|yellow|{2015-12-01 11:00:00, 2015-12-01 12:00:00}|citigroup   |25   |\n",
      "|green |{2015-12-01 11:00:00, 2015-12-01 12:00:00}|citigroup   |2    |\n",
      "|yellow|{2015-12-01 12:00:00, 2015-12-01 13:00:00}|citigroup   |34   |\n",
      "|green |{2015-12-01 12:00:00, 2015-12-01 13:00:00}|citigroup   |1    |\n",
      "|yellow|{2015-12-01 13:00:00, 2015-12-01 14:00:00}|citigroup   |30   |\n",
      "|yellow|{2015-12-01 14:00:00, 2015-12-01 15:00:00}|citigroup   |42   |\n",
      "+------+------------------------------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "citigroup_res = result.filter(f.col('headquarters') == 'citigroup')\n",
    "citigroup_res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Create folders and save the result of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_increment = 360000\n",
    "count = count_increment\n",
    "\n",
    "for row, row1 in zip(goldman_res.collect(), citigroup_res.collect()):\n",
    "    new_path = f'./output-{count}'\n",
    "\n",
    "    os.makedirs(new_path, exist_ok=True)\n",
    "\n",
    "    with open(f\"{new_path}/output-{count}.txt\", \"w\") as file:\n",
    "        file.write(f\"goldman: {str(row['count'])}\\ncitigroup: {row1['count']}\")\n",
    "        \n",
    "    count += count_increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingCount = (                 \n",
    "    filteredDF\n",
    "    .groupBy( \n",
    "      filteredDF.Type,\n",
    "      window(filteredDF.dropoff_datetime, \"10 minutes\"), filteredDF.headquarters)\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# This query stores the aggregation results in memory then visualize it\n",
    "query = (\n",
    "  streamingCount\n",
    "    .writeStream\n",
    "    .format(\"memory\")         # console or memory(= store in-memory table) \n",
    "    .queryName(\"counts\")      # counts = name of the in-memory table\n",
    "    .outputMode(\"complete\")   \n",
    "    .option(\"truncate\", \"false\")\n",
    "    .start()\n",
    ")\n",
    "query.awaitTermination(60)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id):\n",
    "        batch_df.persist()\n",
    "        goldman_trends = detect_trends(batch_df, \"goldman\")\n",
    "        citigroup_trends = detect_trends(batch_df, \"citigroup\")\n",
    "        goldman_trends.show(truncate=False)\n",
    "        citigroup_trends.show(truncate=False)\n",
    "        batch_df.unpersist()\n",
    "\n",
    "def detect_trends(df, headquarters):\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import lag\n",
    "\n",
    "    w = Window.partitionBy(\"headquarters\").orderBy(\"window\")\n",
    "\n",
    "    df = df.filter(col(\"headquarters\") == headquarters)\n",
    "    df = df.withColumn(\"prev_count\", lag(\"count\").over(w))\n",
    "\n",
    "    df = df.filter((col(\"count\") >= 10) & (col(\"prev_count\").isNotNull()))\n",
    "    df = df.filter(col(\"count\") >= 2 * col(\"prev_count\"))\n",
    "\n",
    "    df.selectExpr(\n",
    "        \"headquarters\",\n",
    "        \"count as current_count\",\n",
    "        \"window.start as timestamp\",\n",
    "        \"prev_count\"\n",
    "    ).write.mode(\"append\").json(output_path)\n",
    "\n",
    "    return df.selectExpr(\n",
    "        \"headquarters\",\n",
    "        \"prev_count\",\n",
    "        \"count as current_count\",\n",
    "        \"window.start as timestamp\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
